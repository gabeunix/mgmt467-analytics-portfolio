{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabeunix/mgmt467-analytics-portfolio/blob/main/GWang_Unit2_Lab1_PromptPlusExamples_Colab_Kaggle_GCS_BQ_DQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8rOXS-2XZd8"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "l8rOXS-2XZd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sty69PnaXZeA"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "sty69PnaXZeA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny-Z4zc1XZeA"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "ny-Z4zc1XZeA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-HY9KsCXZeB"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "4-HY9KsCXZeB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABn9O4TWXZeB"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "ABn9O4TWXZeB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tFyVF1rXZeB",
        "outputId": "7b91a0be-4fb5-4628-9172-0647e0a34a73"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: boxwood-veld-471119-r6\n",
            "Project: boxwood-veld-471119-r6 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "boxwood-veld-471119-r6\n"
          ]
        }
      ],
      "source": [
        "# # EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "# #\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "# #\n",
        "# # # Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# # # Done: Auth + Project/Region set"
      ],
      "id": "0tFyVF1rXZeB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6heZyiMXZeC"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "Q6heZyiMXZeC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psrO_gArXZeD"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "psrO_gArXZeD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOlxfDxNXZeD"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "gOlxfDxNXZeD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH7IspSsXZeD"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "vH7IspSsXZeD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "8e0b4507",
        "outputId": "54b22f09-9538-4c5b-b01d-961d38589d01"
      },
      "source": [
        "#EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "!kaggle --version"
      ],
      "id": "8e0b4507",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f87f0cd0-28fb-4c48-b9e1-8aa7d576a4df\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f87f0cd0-28fb-4c48-b9e1-8aa7d576a4df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edksrSLtXZeE"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "edksrSLtXZeE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0777d1c",
        "outputId": "d289a1d8-fa86-4205-ff6b-396b1d803e36"
      },
      "source": [
        "!kaggle --help | head -n 20"
      ],
      "id": "c0777d1c",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqPJ1Y1GXZeE"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "BqPJ1Y1GXZeE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requiring strict 0600 permissions on API tokens means that only the owner of the file can read and write to it. This is a crucial security measure to protect your sensitive API credentials. By setting these permissions, you are preventing other users on the system (if any) or processes running under different user accounts from accessing or potentially stealing your API key. This helps avoid risks such as unauthorized access to your accounts, data breaches, and potential misuse of your API key, which could lead to unexpected costs or other security compromises."
      ],
      "metadata": {
        "id": "KcdUHfobfWCs"
      },
      "id": "KcdUHfobfWCs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUQywA0qXZeE"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "cUQywA0qXZeE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51zNINn1XZeE"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "51zNINn1XZeE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYcvcs0AXZeE",
        "outputId": "eeb35a67-6fc4-4bd6-918e-9111529f2c7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 754MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "#EXAMPLE (from LLM) — Download & unzip (commented)\n",
        "!mkdir -p /content/data/raw\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# List CSV inventory\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "id": "cYcvcs0AXZeE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRheTfhDXZeE"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "yRheTfhDXZeE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGdXIDF4XZeF"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "jGdXIDF4XZeF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xj2w54_XZeF"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "9Xj2w54_XZeF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGKNPvWmXZeF"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "aGKNPvWmXZeF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ussYbt4AXZeF",
        "outputId": "ca451390-e4fb-4199-96ee-abde789bebdb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mgmt467-netflix-3e491f27/...\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-3e491f27/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-3e491f27/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-3e491f27/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-3e491f27/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-3e491f27/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-3e491f27/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 91.1MiB/s\n",
            "Created and uploaded data to GCS bucket: mgmt467-netflix-3e491f27\n",
            "\n",
            "Benefits of staging in GCS:\n",
            "- **Consistent Source:** Provides a stable location for data, unlike temporary Colab storage.\n",
            "- **Versionable:** GCS supports object versioning for easier rollback and auditing.\n",
            "- **Decoupled Storage:** Separates data storage from compute, allowing BigQuery to access data efficiently.\n",
            "- **Auditable:** GCS logs provide an audit trail of data access and modifications.\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "# Create a unique bucket name with a random suffix\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# Create the GCS bucket\n",
        "# Attempting without explicit location due to previous error\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME\n",
        "\n",
        "# Upload all CSV files from the raw data directory to the bucket\n",
        "# Staging data in GCS provides a consistent, versionable source for data loading into BigQuery.\n",
        "# It decouples data storage from the compute layer and allows for easier auditing and recovery.\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://$BUCKET_NAME/netflix/\n",
        "\n",
        "# Print the bucket name\n",
        "print(f\"Created and uploaded data to GCS bucket: {bucket_name}\")\n",
        "print(\"\\nBenefits of staging in GCS:\")\n",
        "print(\"- **Consistent Source:** Provides a stable location for data, unlike temporary Colab storage.\")\n",
        "print(\"- **Versionable:** GCS supports object versioning for easier rollback and auditing.\")\n",
        "print(\"- **Decoupled Storage:** Separates data storage from compute, allowing BigQuery to access data efficiently.\")\n",
        "print(\"- **Auditable:** GCS logs provide an audit trail of data access and modifications.\")"
      ],
      "id": "ussYbt4AXZeF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V30ZxlJlXZeF"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "V30ZxlJlXZeF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a88085",
        "outputId": "b8d3b859-d1c5-442c-f8b1-cb4a0e81ab79"
      },
      "source": [
        "# List objects in the bucket under the 'netflix/' prefix with sizes\n",
        "import os\n",
        "bucket_name = os.environ.get(\"BUCKET_NAME\")\n",
        "if bucket_name:\n",
        "  !gcloud storage ls --readable-sizes gs://$BUCKET_NAME/netflix/\n",
        "else:\n",
        "  print(\"BUCKET_NAME environment variable is not set.\")"
      ],
      "id": "b9a88085",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mgmt467-netflix-3e491f27/netflix/movies.csv\n",
            "gs://mgmt467-netflix-3e491f27/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-3e491f27/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-3e491f27/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-3e491f27/netflix/users.csv\n",
            "gs://mgmt467-netflix-3e491f27/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyqmNTI5XZeF"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "JyqmNTI5XZeF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persistence and Consistency: Data in local Colab storage is temporary and tied to the Colab session. Staging data in GCS provides a persistent and consistent location for your data that is accessible across different sessions and services, making your data pipeline more robust and reproducible.\n",
        "Decoupling of Storage and Compute: Loading directly from Colab couples your data source tightly to the Colab environment. Staging in GCS decouples storage from compute, allowing services like BigQuery to access the data efficiently and independently. This is a more scalable and flexible approach for cloud-based data processing."
      ],
      "metadata": {
        "id": "GG9TjW3-gmdS"
      },
      "id": "GG9TjW3-gmdS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6emSkgsXZeF"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "s6emSkgsXZeF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2FTlhl_XZeF"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "M2FTlhl_XZeF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkkgKEN0XZeF"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "# # DATASET=\"netflix\"\n",
        "# # # Attempt to create; ignore if exists\n",
        "# # !bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "HkkgKEN0XZeF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "639ba6bb",
        "outputId": "3bafb94e-5a34-4199-80a6-86bceb17dbfa"
      },
      "source": [
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists and print a friendly message\n",
        "!bq --location=US mk -d --description \"Netflix dataset for MGMT467\" $DATASET 2> /dev/null || echo \"Dataset '$DATASET' may already exist.\"\n",
        ""
      ],
      "id": "639ba6bb",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'boxwood-veld-471119-r6:netflix' already\n",
            "exists.\n",
            "Dataset '' may already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "\n",
        "import os\n",
        "DATASET=\"netflix\" # Ensure DATASET is defined\n",
        "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"] # Get PROJECT_ID from environment\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(f\"Loading {tbl} from {src}\")\n",
        "  # Use bq load command with specified flags\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts for verification\n",
        "print(\"\\nRow counts for loaded tables:\")\n",
        "for tbl in tables.keys():\n",
        "  # Escape the backticks with backslashes\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM \\`{PROJECT_ID}.{DATASET}.{tbl}\\`\"\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87MHKGBgz_T_",
        "outputId": "757154b3-bc1f-47a4-e8ae-dccc412a31e2"
      },
      "id": "87MHKGBgz_T_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-3e491f27/netflix/users.csv\n",
            "Waiting on bqjob_r2d231617720abc79_0000019a22d003c9_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-3e491f27/netflix/movies.csv\n",
            "Waiting on bqjob_r574cbe027fec1f7c_0000019a22d01c30_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-3e491f27/netflix/watch_history.csv\n",
            "Waiting on bqjob_r286592e0b816f4ae_0000019a22d031f8_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-3e491f27/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r1c8bb4e3784e62fe_0000019a22d04e6c_1 ... (1s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-3e491f27/netflix/search_logs.csv\n",
            "Waiting on bqjob_r434724f2a4e21b8f_0000019a22d06461_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-3e491f27/netflix/reviews.csv\n",
            "Waiting on bqjob_r55410240437260f5_0000019a22d07c66_1 ... (1s) Current status: DONE   \n",
            "\n",
            "Row counts for loaded tables:\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| users      | 20600 |\n",
            "+------------+-------+\n",
            "+------------+------+\n",
            "| table_name |  n   |\n",
            "+------------+------+\n",
            "| movies     | 2080 |\n",
            "+------------+------+\n",
            "+---------------+--------+\n",
            "|  table_name   |   n    |\n",
            "+---------------+--------+\n",
            "| watch_history | 210000 |\n",
            "+---------------+--------+\n",
            "+---------------------+--------+\n",
            "|     table_name      |   n    |\n",
            "+---------------------+--------+\n",
            "| recommendation_logs | 104000 |\n",
            "+---------------------+--------+\n",
            "+-------------+-------+\n",
            "| table_name  |   n   |\n",
            "+-------------+-------+\n",
            "| search_logs | 53000 |\n",
            "+-------------+-------+\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| reviews    | 30900 |\n",
            "+------------+-------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz487OqNXZeG"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "vz487OqNXZeG"
    },
    {
      "cell_type": "code",
      "source": [
        "!bq query --nouse_legacy_sql 'SELECT table_id AS table_name, row_count FROM `{os.environ[\"GOOGLE_CLOUD_PROJECT\"]}.netflix.__TABLES__` WHERE table_id IN (\"users\", \"movies\", \"watch_history\", \"recommendation_logs\", \"search_logs\", \"reviews\")'\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy7vT7A-6RV_",
        "outputId": "4e283947-8f27-4dfe-d14c-878edcf48881"
      },
      "id": "wy7vT7A-6RV_",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on bqjob_r7fd975191096af01_0000019a22e925b7_1 ... (0s) Current status: DONE   \n",
            "+---------------------+-----------+\n",
            "|     table_name      | row_count |\n",
            "+---------------------+-----------+\n",
            "| movies              |      2080 |\n",
            "| recommendation_logs |    104000 |\n",
            "| reviews             |     30900 |\n",
            "| search_logs         |     53000 |\n",
            "| users               |     20600 |\n",
            "| watch_history       |    210000 |\n",
            "+---------------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Hz8Wb3XZeG"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "Z-Hz8Wb3XZeG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schema autodetect is best used for convenience and exploration, such as when you're first profiling a new dataset or for quick, one-off analyses. It's also reliable for self-describing formats like Parquet or Avro. However, you must enforce an explicit schema in all production environments or automated data pipelines. This is critical for reliability and data quality. An explicit schema acts as a strict contract, ensuring type safety (e.g., preventing a STRING from entering an INT column) and rejecting malformed data. Autodetect only samples the first few rows, so it can easily guess the wrong data type, leading to load failures or silent data corruption when different data appears later in the file. Enforcing a schema prevents these errors, improves load performance, and ensures your data is consistent and trustworthy."
      ],
      "metadata": {
        "id": "7letJoZm7B_f"
      },
      "id": "7letJoZm7B_f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATI3CQphXZeG"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "ATI3CQphXZeG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bp4DqKpXZeG"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "1Bp4DqKpXZeG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM9vNjC0XZeG"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "xM9vNjC0XZeG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a7bfe74",
        "outputId": "6b065d7d-dc9b-4a7a-b75d-cb8d8d4e6ada"
      },
      "source": [
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "# Check the schema of the users table\n",
        "!bq show --schema --format=prettyjson {project_id}:netflix.users"
      ],
      "id": "1a7bfe74",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"user_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"email\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"first_name\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"last_name\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"age\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"gender\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"country\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"state_province\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"city\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"subscription_plan\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"subscription_start_date\",\n",
            "    \"type\": \"DATE\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"is_active\",\n",
            "    \"type\": \"BOOLEAN\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"monthly_spend\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"primary_device\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"household_size\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"created_at\",\n",
            "    \"type\": \"TIMESTAMP\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5ea2ce",
        "outputId": "41a475d8-79c2-46c5-fb49-33abde149a96"
      },
      "source": [
        "# Cell 1: Total rows and % missing in country, subscription_plan, age\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM \\`{project_id}.netflix.users\\`\n",
        ")\n",
        "SELECT n,\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "id": "ec5ea2ce",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:18: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3123452141.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3123452141.py:18: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------------------+-------------------------------+-----------------+\n",
            "|   n   | pct_missing_country | pct_missing_subscription_plan | pct_missing_age |\n",
            "+-------+---------------------+-------------------------------+-----------------+\n",
            "| 20600 |                 0.0 |                           0.0 |           11.93 |\n",
            "+-------+---------------------+-------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rZro5BuXZeG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "# # -- Users: % missing per column\n",
        "# # WITH base AS (\n",
        "# #   SELECT COUNT(*) n,\n",
        "# #          COUNTIF(region IS NULL) miss_region,\n",
        "# #          COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "# #          COUNTIF(age_band IS NULL) miss_age\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # )\n",
        "# # SELECT n,\n",
        "# #        ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "# #        ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "# #        ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "# # FROM base;"
      ],
      "id": "1rZro5BuXZeG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fuNBOaeXZeG"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "# # SELECT region,\n",
        "# #        COUNT(*) AS n,\n",
        "# #        ROUND(100*COUNTIF(plan_tier IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # GROUP BY region\n",
        "# # ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "5fuNBOaeXZeG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RI5f2Z0XZeG"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "3RI5f2Z0XZeG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification: Print the three missingness percentages\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM \\`{project_id}.netflix.users\\`\n",
        ")\n",
        "SELECT ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\"\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_hhErw28Y2K",
        "outputId": "c2a78bee-5ebe-40ec-d595-372b1e5edca0"
      },
      "id": "1_hhErw28Y2K",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-689529737.py:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-689529737.py:17: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------------------+-----------------+\n",
            "| pct_missing_country | pct_missing_subscription_plan | pct_missing_age |\n",
            "+---------------------+-------------------------------+-----------------+\n",
            "|                 0.0 |                           0.0 |           11.93 |\n",
            "+---------------------+-------------------------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP8g1V0NXZeG"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "id": "AP8g1V0NXZeG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the query results, the age column is the one with missing data, at 11.93%. The country and subscription_plan columns have no missing data (0.0%)\n",
        "\n",
        "MCAR (Missing Completely at Random): The fact that the data is missing is completely random. It has nothing to do with any other column or the missing age value itself. (e.g., a database glitch randomly deleted 11.93% of age entries).\n",
        "\n",
        "MAR (Missing at Random): The missingness is related to another column in the dataset. (e.g., users from a specific country or on a free_tier plan are less likely to provide their age).\n",
        "\n",
        "MNAR (Missing Not at Random): The missingness is related to the value of the missing data itself. (e.g., people who are very young or very old are less likely to provide their age)."
      ],
      "metadata": {
        "id": "sssteRcj8nmG"
      },
      "id": "sssteRcj8nmG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7OVV0WQXZeG"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "X7OVV0WQXZeG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4FkeEqvXZeG"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "Q4FkeEqvXZeG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI37FA-EXZeH"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "# # SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "# # GROUP BY user_id, movie_id, event_ts, device_type\n",
        "# # HAVING dup_count > 1\n",
        "# # ORDER BY dup_count DESC\n",
        "# # LIMIT 20;"
      ],
      "id": "pI37FA-EXZeH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIAr-cDXXZeN"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "# # SELECT * EXCEPT(rk) FROM (\n",
        "# #   SELECT h.*,\n",
        "# #          ROW_NUMBER() OVER (\n",
        "# #            PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "# #            ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "# #          ) AS rk\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        "# # )\n",
        "# # WHERE rk = 1;"
      ],
      "id": "FIAr-cDXXZeN"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "!bq show --schema --format=prettyjson {project_id}:netflix.watch_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW92pLHj9R-P",
        "outputId": "dc324af9-3a4c-4a60-9a2b-bf3c409d2445"
      },
      "id": "hW92pLHj9R-P",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"session_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"user_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"movie_id\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"watch_date\",\n",
            "    \"type\": \"DATE\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"device_type\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"watch_duration_minutes\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"progress_percentage\",\n",
            "    \"type\": \"FLOAT\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"action\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"quality\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"location_country\",\n",
            "    \"type\": \"STRING\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"is_download\",\n",
            "    \"type\": \"BOOLEAN\"\n",
            "  },\n",
            "  {\n",
            "    \"mode\": \"NULLABLE\",\n",
            "    \"name\": \"user_rating\",\n",
            "    \"type\": \"INTEGER\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT user_id,\n",
        "       movie_id,\n",
        "       watch_date,\n",
        "       device_type,\n",
        "       COUNT(*) AS duplicate_count\n",
        "FROM \\`{project_id}.netflix.watch_history\\`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING COUNT(*) > 1\n",
        "ORDER BY duplicate_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5tPgELu9e2G",
        "outputId": "5afc3cad-f136-4e60-ce9c-c0ef66174b92"
      },
      "id": "H5tPgELu9e2G",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-1430081540.py:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-1430081540.py:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+------------+-------------+-----------------+\n",
            "|  user_id   |  movie_id  | watch_date | device_type | duplicate_count |\n",
            "+------------+------------+------------+-------------+-----------------+\n",
            "| user_03310 | movie_0640 | 2024-09-08 | Smart TV    |               8 |\n",
            "| user_00391 | movie_0893 | 2024-08-26 | Laptop      |               8 |\n",
            "| user_02822 | movie_0009 | 2025-08-30 | Desktop     |               6 |\n",
            "| user_09972 | movie_0536 | 2025-07-16 | Laptop      |               6 |\n",
            "| user_01292 | movie_0231 | 2024-07-05 | Laptop      |               6 |\n",
            "| user_06103 | movie_0113 | 2025-04-08 | Laptop      |               6 |\n",
            "| user_01807 | movie_0921 | 2025-01-30 | Laptop      |               6 |\n",
            "| user_07981 | movie_0094 | 2025-11-08 | Laptop      |               6 |\n",
            "| user_08826 | movie_0133 | 2025-04-11 | Desktop     |               6 |\n",
            "| user_07738 | movie_0793 | 2025-07-28 | Desktop     |               6 |\n",
            "| user_01383 | movie_0015 | 2025-04-29 | Desktop     |               6 |\n",
            "| user_04513 | movie_0564 | 2024-06-11 | Mobile      |               6 |\n",
            "| user_02369 | movie_0673 | 2024-07-04 | Mobile      |               6 |\n",
            "| user_00249 | movie_0203 | 2024-08-31 | Laptop      |               6 |\n",
            "| user_03898 | movie_0500 | 2025-07-29 | Desktop     |               6 |\n",
            "| user_04698 | movie_0482 | 2025-01-18 | Mobile      |               6 |\n",
            "| user_07617 | movie_0785 | 2024-07-14 | Desktop     |               6 |\n",
            "| user_08681 | movie_0332 | 2024-06-13 | Laptop      |               6 |\n",
            "| user_06462 | movie_0588 | 2025-02-10 | Laptop      |               6 |\n",
            "| user_02284 | movie_0914 | 2024-12-30 | Laptop      |               6 |\n",
            "+------------+------------+------------+-------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Create deduplicated watch_history table\n",
        "# Strategy: Keep one row per (user_id, movie_id, watch_date, device_type) group\n",
        "# Preference: Higher progress_percentage first, then higher watch_duration_minutes\n",
        "# This removes duplicates while preserving the most complete viewing record\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE \\`{project_id}.netflix.watch_history_dedup\\` AS\n",
        "SELECT * EXCEPT(row_num)\n",
        "FROM (\n",
        "  SELECT *,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS row_num\n",
        "  FROM \\`{project_id}.netflix.watch_history\\`\n",
        ")\n",
        "WHERE row_num = 1\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe8AJuCA9g0W",
        "outputId": "45b16af9-d622-48bf-a659-d32870d2054b"
      },
      "id": "Oe8AJuCA9g0W",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:20: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-2946221543.py:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \n",
            "/tmp/ipython-input-2946221543.py:20: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-2946221543.py:20: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on bqjob_r2f59de04178455c5_0000019a22f6ab43_1 ... (1s) Current status: DONE   \n",
            "Created boxwood-veld-471119-r6.netflix.watch_history_dedup\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JWyiUUPXZeN"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "_JWyiUUPXZeN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification: Compare row counts before and after deduplication\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  'Original' AS table_name,\n",
        "  COUNT(*) AS row_count\n",
        "FROM \\`{project_id}.netflix.watch_history\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'Deduplicated' AS table_name,\n",
        "  COUNT(*) AS row_count\n",
        "FROM \\`{project_id}.netflix.watch_history_dedup\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'Duplicates Removed' AS table_name,\n",
        "  (SELECT COUNT(*) FROM \\`{project_id}.netflix.watch_history\\`) -\n",
        "  (SELECT COUNT(*) FROM \\`{project_id}.netflix.watch_history_dedup\\`) AS row_count\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\"\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el9RwXkN94y6",
        "outputId": "a663733e-6a36-44d1-8087-c484a8d176d2"
      },
      "id": "el9RwXkN94y6",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:28: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-211677007.py:28: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-211677007.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-211677007.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-211677007.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-211677007.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|     table_name     | row_count |\n",
            "+--------------------+-----------+\n",
            "| Deduplicated       |    100000 |\n",
            "| Duplicates Removed |    110000 |\n",
            "| Original           |    210000 |\n",
            "+--------------------+-----------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdlgR7RzXZeN"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?"
      ],
      "id": "xdlgR7RzXZeN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicates primarily arise from two sources: system-generated errors and natural user behavior. System-generated duplicates are technical flaws, such as an ETL job failing and re-running, which re-loads the same data, or a streaming system guaranteeing \"at-least-once\" delivery, which might send the same event twice to prevent data loss. Natural duplicates, on the other hand, are caused by user actions, like a person impatiently clicking a \"submit\" button multiple times or signing up for an account twice with different email addresses.\n",
        "\n",
        "This duplication severely corrupts analytics and machine learning. For KPIs, duplicates directly inflate metrics, leading to flawed business decisions; in your case, the 210,000 \"Original\" watch_history events would report user engagement as more than double the true 100,000 \"Deduplicated\" events. For ML labels, the impact is even more damaging. Duplicates in the training set cause overfitting, as the model learns to memorize these repeated examples instead of generalizing. Even worse, if duplicates leak into both the training and test sets, the model effectively \"cheats\" by training on the answers, leading to a completely fake and inflated accuracy score that will fail in a real-world scenario."
      ],
      "metadata": {
        "id": "8Zxxt18x-CP1"
      },
      "id": "8Zxxt18x-CP1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8BFMUMDXZeN"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "E8BFMUMDXZeN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iad4j4qxXZeO"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "iad4j4qxXZeO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AqTTep4XZeO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "# # WITH dist AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(1)] AS q1,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(3)] AS q3\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # bounds AS (\n",
        "# #   SELECT q1, q3, (q3-q1) AS iqr,\n",
        "# #          q1 - 1.5*(q3-q1) AS lo,\n",
        "# #          q3 + 1.5*(q3-q1) AS hi\n",
        "# #   FROM dist\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi) AS outliers,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h\n",
        "# # CROSS JOIN bounds b;"
      ],
      "id": "5AqTTep4XZeO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK1I6uriXZeO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust` AS\n",
        "# # WITH q AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(1)]  AS p01,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(98)] AS p99\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   h.*,\n",
        "# #   GREATEST(q.p01, LEAST(q.p99, h.minutes_watched)) AS minutes_watched_capped\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h, q;\n",
        "# #\n",
        "# # -- Quantiles before vs after\n",
        "# # WITH before AS (\n",
        "# #   SELECT 'before' AS which, APPROX_QUANTILES(minutes_watched, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # after AS (\n",
        "# #   SELECT 'after' AS which, APPROX_QUANTILES(minutes_watched_capped, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        "# # )\n",
        "# # SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "id": "yK1I6uriXZeO"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT q1, q3, (q3-q1) AS iqr,\n",
        "         q1 - 1.5*(q3-q1) AS lo,\n",
        "         q3 + 1.5*(q3-q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100*COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\` h\n",
        "CROSS JOIN bounds b\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFl1twJ0-s90",
        "outputId": "b17ef158-8e4e-47f7-ecf4-1839238283d8"
      },
      "id": "NFl1twJ0-s90",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:27: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:27: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-2869115231.py:27: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-2869115231.py:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-2869115231.py:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+--------------+\n",
            "| outliers | total  | pct_outliers |\n",
            "+----------+--------+--------------+\n",
            "|     3521 | 100000 |         3.52 |\n",
            "+----------+--------+--------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "sql_create_table = f\"\"\"\n",
        "CREATE OR REPLACE TABLE \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust\\` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)]  AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "  FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_minutes_capped\n",
        "FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\` h, q\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_create_table}\"\n",
        "\n",
        "print(\"\\nQuantiles before vs after capping:\")\n",
        "\n",
        "sql_quantiles = f\"\"\"\n",
        "WITH before AS (\n",
        "  SELECT 'before' AS which, APPROX_QUANTILES(watch_duration_minutes, 5) AS q\n",
        "  FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\`\n",
        "),\n",
        "after AS (\n",
        "  SELECT 'after' AS which, APPROX_QUANTILES(watch_duration_minutes_capped, 5) AS q\n",
        "  FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust\\`\n",
        ")\n",
        "SELECT * FROM before UNION ALL SELECT * FROM after\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_quantiles}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAOxVnMtAJ-O",
        "outputId": "986f7b76-784c-454b-ccfa-438ed1cd52e5"
      },
      "id": "oAOxVnMtAJ-O",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:16: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:34: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:31: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:31: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:16: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:34: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:31: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:31: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-2370885154.py:16: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \n",
            "/tmp/ipython-input-2370885154.py:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-2370885154.py:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-2370885154.py:15: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-2370885154.py:34: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-2370885154.py:31: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-2370885154.py:31: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting on bqjob_r6abdbe335aea0458_0000019a2300b03f_1 ... (1s) Current status: DONE   \n",
            "Created boxwood-veld-471119-r6.netflix.watch_history_robust\n",
            "\n",
            "\n",
            "Quantiles before vs after capping:\n",
            "+--------+---------------------------------------------+\n",
            "| which  |                      q                      |\n",
            "+--------+---------------------------------------------+\n",
            "| after  | [\"4.4\",\"24.6\",\"41.5\",\"61.5\",\"92.0\",\"356.3\"] |\n",
            "| before | [\"0.2\",\"24.8\",\"41.7\",\"61.2\",\"91.7\",\"799.3\"] |\n",
            "+--------+---------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daKZDJsaXZeO"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "daKZDJsaXZeO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verification: Show min/median/max before vs after capping\n",
        "import os\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  'Before (Original)' AS version,\n",
        "  MIN(watch_duration_minutes) AS min_val,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_val,\n",
        "  MAX(watch_duration_minutes) AS max_val\n",
        "FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'After (Capped)' AS version,\n",
        "  MIN(watch_duration_minutes_capped) AS min_val,\n",
        "  APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_val,\n",
        "  MAX(watch_duration_minutes_capped) AS max_val\n",
        "FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust\\`\n",
        "\n",
        "ORDER BY version DESC\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRF-dvBdAs2C",
        "outputId": "ae96850f-bd0b-450c-ccf8-91940edf7794"
      },
      "id": "rRF-dvBdAs2C",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:28: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:28: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-791161878.py:28: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-791161878.py:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-791161878.py:22: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+---------+------------+---------+\n",
            "|      version      | min_val | median_val | max_val |\n",
            "+-------------------+---------+------------+---------+\n",
            "| Before (Original) |     0.2 |       50.8 |   799.3 |\n",
            "| After (Capped)    |     4.4 |       51.4 |   356.3 |\n",
            "+-------------------+---------+------------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62X-aPLpXZeO"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "id": "62X-aPLpXZeO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capping (or winsorizing) can be harmful when the outliers are not data errors but are legitimate, meaningful events. This is especially damaging in anomaly or fraud detection, where the extreme outlier is the signal you're trying to find; capping it effectively deletes the data you need most. It also artificially compresses the data's variance and distorts its true distribution, causing models to underestimate the full range of real-world possibilities and the true level of risk (e.g., in financial or load-capacity models).\n",
        "\n",
        "Random Forests (and other decision tree-based models) are much less sensitive to outliers. This is because they work by splitting data based on rank and order (e.g., \"is age > 65?\"), not by its magnitude (e.g., \"how far is age from the mean?\"). An extreme outlier (like an age of 500) will simply be sorted into the same group as all other high values and cannot \"pull\" or \"skew\" the model's decision boundaries, unlike in"
      ],
      "metadata": {
        "id": "RZhli4pVBHZi"
      },
      "id": "RZhli4pVBHZi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiT9-zQbXZeO"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "YiT9-zQbXZeO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SUV1umGXZeO"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "9SUV1umGXZeO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZGLR2cXZeO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(minutes_watched > 8*60) AS sessions_over_8h,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(minutes_watched > 8*60)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`;"
      ],
      "id": "oQZGLR2cXZeO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xJ4lPF8XZeO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #           CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100) AS extreme_age_rows,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #                     CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`;"
      ],
      "id": "-xJ4lPF8XZeO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w0tK-TxXZeO"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(duration_min < 15) AS titles_under_15m,\n",
        "# #   COUNTIF(duration_min > 8*60) AS titles_over_8h,\n",
        "# #   COUNT(*) AS total\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "6w0tK-TxXZeO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Create and summarize flag_binge for sessions > 8 hours (480 minutes)\n",
        "# Identifies potential binge-watching behavior in watch_history_robust\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes_capped > 480) AS binge_sessions,\n",
        "  COUNT(*) AS total_sessions,\n",
        "  ROUND(100 * COUNTIF(watch_duration_minutes_capped > 480) / COUNT(*), 2) AS pct_binge,\n",
        "  AVG(CASE WHEN watch_duration_minutes_capped > 480 THEN watch_duration_minutes_capped END) AS avg_binge_duration,\n",
        "  MAX(watch_duration_minutes_capped) AS max_binge_duration\n",
        "FROM \\`{project_id}.netflix.watch_history_robust\\`\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNiZkfzABhFc",
        "outputId": "bf6f22f9-10f5-4f3d-b395-0d1097db2b0a"
      },
      "id": "MNiZkfzABhFc",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:14: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-541088457.py:21: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-541088457.py:14: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+----------------+-----------+--------------------+--------------------+\n",
            "| binge_sessions | total_sessions | pct_binge | avg_binge_duration | max_binge_duration |\n",
            "+----------------+----------------+-----------+--------------------+--------------------+\n",
            "|              0 |         100000 |       0.0 |               NULL |              356.3 |\n",
            "+----------------+----------------+-----------+--------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Create and summarize flag_age_extreme for users with age <10 or >100\n",
        "# Flag extreme/suspicious ages that may indicate data quality issues\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_users,\n",
        "  COUNT(*) AS total_users_with_age,\n",
        "  ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct_extreme_age,\n",
        "  COUNTIF(age < 10) AS users_under_10,\n",
        "  COUNTIF(age > 100) AS users_over_100,\n",
        "  MIN(age) AS min_age,\n",
        "  MAX(age) AS max_age\n",
        "FROM \\`{project_id}.netflix.users\\`\n",
        "WHERE age IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUe3FLlOBkXa",
        "outputId": "3922cb1d-4eae-4f6c-efab-4964005d5b1b"
      },
      "id": "eUe3FLlOBkXa",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:26: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:26: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:17: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3560977528.py:26: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3560977528.py:17: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------------------+-----------------+----------------+----------------+---------+---------+\n",
            "| extreme_age_users | total_users_with_age | pct_extreme_age | users_under_10 | users_over_100 | min_age | max_age |\n",
            "+-------------------+----------------------+-----------------+----------------+----------------+---------+---------+\n",
            "|               358 |                18142 |            1.97 |            336 |             22 |    -7.0 |   109.0 |\n",
            "+-------------------+----------------------+-----------------+----------------+----------------+---------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Compute and summarize flag_duration_anomaly for movies with durations < 15 or > 480 minutes\n",
        "# Flag movies with potentially anomalous durations (very short or very long)\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15 OR duration_minutes > 480) AS duration_anomalies,\n",
        "  COUNT(*) AS total_movies,\n",
        "  ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2)\n",
        "    AS pct_duration_anomalies,\n",
        "  COUNTIF(duration_minutes < 15)  AS movies_under_15_min,\n",
        "  COUNTIF(duration_minutes > 480) AS movies_over_480_min\n",
        "FROM \\`{project_id}.netflix.movies\\`\n",
        "WHERE duration_minutes IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsVBNu1rBsGL",
        "outputId": "a2b318b9-f3df-4cf7-8516-340bc700a470"
      },
      "id": "jsVBNu1rBsGL",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:16: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:16: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3479281729.py:24: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3479281729.py:16: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+------------------------+---------------------+---------------------+\n",
            "| duration_anomalies | total_movies | pct_duration_anomalies | movies_under_15_min | movies_over_480_min |\n",
            "+--------------------+--------------+------------------------+---------------------+---------------------+\n",
            "|                 46 |         2080 |                   2.21 |                  24 |                  22 |\n",
            "+--------------------+--------------+------------------------+---------------------+---------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ9Uaw9kXZeO"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "XJ9Uaw9kXZeO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Compact summary of all data quality and behavioral flags (binge, age, duration)\n",
        "import os\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_query = f\"\"\"\n",
        "SELECT 'flag_binge' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(watch_duration_minutes_capped > 480) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM \\`{project_id}.netflix.watch_history_robust\\`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT 'flag_age_extreme' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM \\`{project_id}.netflix.users\\`\n",
        "WHERE age IS NOT NULL\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT 'flag_duration_anomaly' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM \\`{project_id}.netflix.movies\\`\n",
        "WHERE duration_minutes IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "!bq query --nouse_legacy_sql \"{sql_query}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQQgw_bwB9IE",
        "outputId": "aa6ffd99-b56a-4a9b-f2d1-4c0e6355b345"
      },
      "id": "IQQgw_bwB9IE",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:26: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:26: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3912147049.py:26: SyntaxWarning: invalid escape sequence '\\`'\n",
            "/tmp/ipython-input-3912147049.py:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-3912147049.py:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n",
            "/tmp/ipython-input-3912147049.py:23: SyntaxWarning: invalid escape sequence '\\`'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-------------+\n",
            "|       flag_name       | pct_of_rows |\n",
            "+-----------------------+-------------+\n",
            "| flag_binge            |         0.0 |\n",
            "| flag_age_extreme      |        1.97 |\n",
            "| flag_duration_anomaly |        2.21 |\n",
            "+-----------------------+-------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWRGyguZXZeO"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "id": "LWRGyguZXZeO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common anomaly flag is flag_duration_anomaly, at 2.21%. As for which to keep, both flags are valuable as features, but for different reasons. flag_duration_anomaly: This should be kept as a feature to capture unusual content characteristics. A movie with an abnormally short or long duration is an outlier that could skew recommendation models or average watch-time calculations. By flagging it, a model can learn whether this \"unusualness\" itself affects user behavior or satisfaction. flag_binge: This is a critical feature to keep because it captures a highly relevant user behavior. Even though it's 0.0% in this specific dataset, this flag is a powerful signal that would be highly predictive for modeling engagement, churn, or subscription upgrades. It's essential to include it for future data where binge-watching does occur."
      ],
      "metadata": {
        "id": "jMUrcQuvDaiv"
      },
      "id": "jMUrcQuvDaiv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCNa1yThXZeP"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "lCNa1yThXZeP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA6jnh6CXZeP"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "EA6jnh6CXZeP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xM2mUloXZeP"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "5xM2mUloXZeP"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}